{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957939a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "import transformer_lens\n",
    "torch.set_default_device(\"cuda\")\n",
    "import pandas as pd\n",
    "from src.utils import get_w_vo\n",
    "from src.maps import MAPS\n",
    "from tabulate import tabulate\n",
    "\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06fe6999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-xl into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "model_name = r\"gpt2-xl\"\n",
    "model = transformer_lens.HookedTransformer.from_pretrained_no_processing(model_name, device_map=\"auto\")\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "model.eval()\n",
    "state_dict = model.state_dict()\n",
    "cfg = model.cfg\n",
    "is_gqa = cfg.n_key_value_heads != None\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_gpus = torch.cuda.device_count()\n",
    "last_device = torch.device(f\"cuda:{num_gpus-1}\" if torch.cuda.is_available() else \"cpu\")\n",
    "def first_mlp(x):\n",
    "    return model.blocks[0].mlp.to(device)(model.blocks[0].ln2.to(device)(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11b003cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 26\n",
    "head = 2\n",
    "maps = MAPS(model, tokenizer)\n",
    "apply_first_mlp = True\n",
    "k_salient_tokens = 15\n",
    "k_mappings = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aa92323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token          Mappings\n",
      "' Jedi'        ' lightsaber', ' Jedi', ' Kenobi', ' droid', ' Skywalker'\n",
      "' lightsaber'  ' lightsaber', ' Jedi', ' Kenobi', ' Skywalker', ' Sith'\n",
      "' galactic'    ' Galactic', ' galactic', ' starship', ' galaxy', ' droid'\n",
      "' Starfleet'   ' galactic', ' Starfleet', ' starship', ' Galactic', ' interstellar'\n",
      "' Klingon'     ' starship', ' Starfleet', ' Klingon', ' Trek', ' Starship'\n",
      "' starship'    ' starship', ' Galactic', ' galactic', ' interstellar', ' Planetary'\n",
      "' Skyrim'      ' Skyrim', ' Magicka', ' Bethesda', ' Elven', ' Hearth'\n",
      "' Darth'       ' Jedi', ' lightsaber', ' Kenobi', ' Darth', ' Sith'\n",
      "' galaxy'      ' Galactic', ' galactic', ' starship', ' galaxy', ' droid'\n",
      "' Fairy'       ' Fairy', ' Magical', ' fairy', ' Pokémon', ' Cinderella'\n",
      "' droid'       ' droid', ' Kenobi', ' Galactic', ' lightsaber', ' Jedi'\n",
      "'Pokémon'      ' Pokémon', 'Pokémon', ' Pikachu', ' Poké', ' Pokemon'\n",
      "' Sith'        ' Sith', ' Jedi', ' lightsaber', ' Kenobi', ' Mandal'\n",
      "' Elven'       ' Elven', ' elves', ' Dwar', ' Elves', ' wizard'\n",
      "' Warhammer'   ' Warhammer', ' Imperium', ' Horus', ' Imperial', ' Inquisitor'\n"
     ]
    }
   ],
   "source": [
    "salient_tokens_decoded, salient_mappings_decoded = maps.get_salient_operations(layer, head, k_salient_tokens, k_mappings, apply_first_mlp)\n",
    "table = list(zip(salient_tokens_decoded, [\", \".join(m) for m in salient_mappings_decoded]))\n",
    "print(tabulate(table, headers=[\"Token\", \"Mappings\"], tablefmt=\"plain\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8b3433e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mapping_str(salient_tokens_decoded, salient_mappings_decoded):\n",
    "    mappings_str = \"\"\n",
    "    for ix,token in enumerate(salient_tokens_decoded):\n",
    "        if ix > 0:\n",
    "            mappings_str += \"\\n\"\n",
    "        mappings_str += f\"{token}: \"\n",
    "        mappings_str += \",\".join(salient_mappings_decoded[ix])\n",
    "    return mappings_str\n",
    "\n",
    "def plug_in_prompt(salient_tokens_decoded, salient_mappings_decoded):\n",
    "    \n",
    "    prompt = f\"\"\"Below you are given a list of input strings, and a list of mappings: each mapping is between an input string and a list of 5 strings. \n",
    "Mappings are provided in the format \"s: t1, t2, t3, t4, t5\" where each of s, t1, t2, t3, t4, t5 is a short string, typically corresponding to a single word or a sub-word.\n",
    "Your goal is to describe shortly and simply the inputs and the function that produces these mappings. To perform the task, look for semantic and textual patterns. \n",
    "For example, input tokens 'water','ice','freeze' are water-related, and a mapping ('fire':'f') is from a word to its first letter.\n",
    "As a final response, suggest the most clear patterns observed or indicate that no clear pattern is visible (write only the word \"Unclear\").\n",
    "Your response should be a vaild json, with the following keys: \n",
    "\"Reasoning\": your reasoning.\n",
    "\"Input strings\": One sentence describing the input strings (or \"Unclear\").\n",
    "\"Observed pattern\": One sentence describing the most clear patterns observed (or \"Unclear\").\n",
    "\n",
    "The input strings are:\n",
    "{salient_tokens_decoded}\n",
    "\n",
    "The mappings are: \n",
    "{get_mapping_str(salient_tokens_decoded, salient_mappings_decoded)}\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d852a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below you are given a list of input strings, and a list of mappings: each mapping is between an input string and a list of 5 strings. \n",
      "Mappings are provided in the format \"s: t1, t2, t3, t4, t5\" where each of s, t1, t2, t3, t4, t5 is a short string, typically corresponding to a single word or a sub-word.\n",
      "Your goal is to describe shortly and simply the inputs and the function that produces these mappings. To perform the task, look for semantic and textual patterns. \n",
      "For example, input tokens 'water','ice','freeze' are water-related, and a mapping ('fire':'f') is from a word to its first letter.\n",
      "As a final response, suggest the most clear patterns observed or indicate that no clear pattern is visible (write only the word \"Unclear\").\n",
      "Your response should be a vaild json, with the following keys: \n",
      "\"Reasoning\": your reasoning.\n",
      "\"Input strings\": One sentence describing the input strings (or \"Unclear\").\n",
      "\"Observed pattern\": One sentence describing the most clear patterns observed (or \"Unclear\").\n",
      "\n",
      "The input strings are:\n",
      "[' Jedi', ' lightsaber', ' galactic', ' Starfleet', ' Klingon', ' starship', ' Skyrim', ' Darth', ' galaxy', ' Fairy', ' droid', 'Pokémon', ' Sith', ' Elven', ' Warhammer']\n",
      "\n",
      "The mappings are: \n",
      " Jedi:  lightsaber, Jedi, Kenobi, droid, Skywalker\n",
      " lightsaber:  lightsaber, Jedi, Kenobi, Skywalker, Sith\n",
      " galactic:  Galactic, galactic, starship, galaxy, droid\n",
      " Starfleet:  galactic, Starfleet, starship, Galactic, interstellar\n",
      " Klingon:  starship, Starfleet, Klingon, Trek, Starship\n",
      " starship:  starship, Galactic, galactic, interstellar, Planetary\n",
      " Skyrim:  Skyrim, Magicka, Bethesda, Elven, Hearth\n",
      " Darth:  Jedi, lightsaber, Kenobi, Darth, Sith\n",
      " galaxy:  Galactic, galactic, starship, galaxy, droid\n",
      " Fairy:  Fairy, Magical, fairy, Pokémon, Cinderella\n",
      " droid:  droid, Kenobi, Galactic, lightsaber, Jedi\n",
      "Pokémon:  Pokémon,Pokémon, Pikachu, Poké, Pokemon\n",
      " Sith:  Sith, Jedi, lightsaber, Kenobi, Mandal\n",
      " Elven:  Elven, elves, Dwar, Elves, wizard\n",
      " Warhammer:  Warhammer, Imperium, Horus, Imperial, Inquisitor\n",
      "\n"
     ]
    }
   ],
   "source": [
    "salient_tokens_decoded_ = [tok[1:-1] for tok in salient_tokens_decoded]\n",
    "salient_mappings_decoded_ = [[tok[1:-1] for tok in tok_lst] for tok_lst in salient_mappings_decoded]\n",
    "prompt = plug_in_prompt(salient_tokens_decoded_, salient_mappings_decoded_)\n",
    "print(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
